{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import maxsize #para imprimir arrays completos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing #para normalizar datos\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_validate, StratifiedKFold\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.metrics import roc_curve, auc, precision_score, recall_score\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Método para imprimir información básica de las columnas del dataframe\n",
    "def getInfoByColumn(df):\n",
    "    \n",
    "    for column in df:\n",
    "        \n",
    "        InfoBasica = df[column].describe()\n",
    "        \n",
    "        uniqueValuesCount = len(df[column].unique())\n",
    "        \n",
    "        #si la columna tiene menos de 10 valores, los imprimimos sin problemas\n",
    "        #si tiene más truncamos el texto para simplificar la lectura\n",
    "        if (uniqueValuesCount < 10):\n",
    "            \n",
    "            ShowUnique = 'Show Unique  ' + str(df[column].unique()).strip('[]')\n",
    "        else:\n",
    "            ShowUnique = 'Show Unique  ' + str(df[column].unique()[0:30]).strip('[]') + ',etc...'\n",
    "        \n",
    "        print('Información columna: {} \\n''---------------\\n{}'.format(column, InfoBasica))\n",
    "        print('{}''\\n'.format(ShowUnique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Método para graficar un histograma por cada columna del dataset\n",
    "def getHistogramByColumn(df):\n",
    "    for column in df:\n",
    "\n",
    "        #Gráfica Histograma:\n",
    "        Histograma = df[column].hist(grid=False, color='indigo', bins=10, xlabelsize=10, xrot=45)\n",
    "        \n",
    "        #Título y nombre de ejes: \n",
    "        plt.xlabel(column, fontsize= 13, color='green')\n",
    "        plt.ylabel('Freq.',fontsize= 13, color='green')\n",
    "        plt.title('Columna: ' + column, fontsize= 20, color='mediumslateblue')\n",
    "        \n",
    "        plt.legend(labels=df[column],  loc='upper right', fontsize='small',bbox_to_anchor=(1.3, 1))\n",
    "        plt.show()\n",
    "        print (Histograma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Método para obtener datos estadísticos de cada columna del dataframe\n",
    "def getStatisticForEachColumn(df):\n",
    "    \n",
    "    for column in df:\n",
    "        \n",
    "        STD = df[column].std()\n",
    "        \n",
    "        MEAN = df[column].mean()\n",
    "        \n",
    "        VAR =  df[column].var()\n",
    "        \n",
    "        print('Statistics mesures from:{}\\n-----------------------------\\nSTD:{}\\nVAR: {}\\nMean: {}\\n'.format(column, STD, VAR, MEAN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Método para generar un gráfico de correlación de las variables del data frame\n",
    "def plot_corr(df,size=10):\n",
    "    '''Function plots a graphical correlation matrix for each pair of columns in the dataframe.\n",
    "\n",
    "    Input:\n",
    "        df: pandas DataFrame\n",
    "        size: vertical and horizontal size of the plot'''\n",
    "\n",
    "    corr = df\n",
    "    fig, ax = plt.subplots(figsize=(size, size),)\n",
    "    ax.matshow(corr)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns);\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos relacionados al modelo KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Método para obtener el hiperparámetro K más óptimo\n",
    "#useStandarization determina si se aplica o no la estandarización a los valores de los datos \n",
    "#retorna un dataframe con el score para todos los K's, cuya cantidad es determinada por quantityK\n",
    "def getScoresForHyperparameterK(quantityK, stepK, model_X_train, model_y_train, kFold_N_Splits=5, KFold_shuffle=True, useStandarization=False):\n",
    "    kf = StratifiedKFold(n_splits=kFold_N_Splits, shuffle=KFold_shuffle, random_state=12)\n",
    "\n",
    "    scores_para_df = []\n",
    "    \n",
    "    #si viene configurado, se realiza la estandarización de los valores\n",
    "    if(useStandarization):\n",
    "        scaler = StandardScaler()\n",
    "        model_X_train = scaler.fit_transform(model_X_train)\n",
    "\n",
    "    for i in range(1, quantityK+1, stepK):\n",
    "\n",
    "        # En cada iteración instanciamos el modelo con un hiperparámetro distinto\n",
    "        model = KNeighborsClassifier(n_neighbors=i)\n",
    "\n",
    "        # cross_val_scores nos devuelve un array de 5 resultados,\n",
    "        # uno por cada partición que hizo automáticamente CV\n",
    "        cv_scores = cross_val_score(model, model_X_train, model_y_train, cv=kf)\n",
    "\n",
    "        # Para cada valor de n_neighbours, creo un diccionario con el valor\n",
    "        # de n_neighbours y la media y el desvío de los scores.\n",
    "        dict_row_score = {'score_medio':np.mean(cv_scores),\\\n",
    "                          'score_std':np.std(cv_scores), 'n_neighbours':i}\n",
    "\n",
    "        # Guardo cada uno en la lista de diccionarios\n",
    "        scores_para_df.append(dict_row_score)\n",
    "    \n",
    "    dfResult = pd.DataFrame(scores_para_df)\n",
    "    dfResult['limite_inferior'] = dfResult['score_medio'] - dfResult['score_std']\n",
    "    dfResult['limite_superior'] = dfResult['score_medio'] + dfResult['score_std']\n",
    "    \n",
    "    return dfResult\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKNNPredictions(X_train, y_train, X_test, y_test, K, useStandarization = False):\n",
    "    \n",
    "    if(useStandarization):\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train) \n",
    "        X_test = scaler.transform(X_test) \n",
    "    \n",
    "    model = KNeighborsClassifier(n_neighbors=K)   \n",
    "    \n",
    "    print(model)\n",
    "    \n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    return y_pred, model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metodos relacionados al modelo Regresion Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Método para obtener el hiperparámetro C más óptimo\n",
    "#useStandarization determina si se aplica o no la estandarización a los valores de los datos\n",
    "#retorna un dataframe con el score para todos los C's del array valoresPosiblesC\n",
    "def getScoresForHyperparameterC(valoresPosiblesC, model_X_train, model_y_train, kFold_N_Splits=5, KFold_shuffle=True, useStandarization=False):\n",
    "    kf = KFold(n_splits=kFold_N_Splits, shuffle=KFold_shuffle, random_state=12)\n",
    "\n",
    "    scores_para_df = []\n",
    "    \n",
    "    #si viene configurado, se realiza la estandarización de los valores\n",
    "    if(useStandarization):\n",
    "        scaler = StandardScaler()\n",
    "        model_X_train = scaler.fit_transform(model_X_train)\n",
    "\n",
    "    for i in valoresPosiblesC:\n",
    "        \n",
    "        #para evitar el warning\n",
    "        #C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
    "        #se usa el parametro solver='lbfgs'\n",
    "        model = linear_model.LogisticRegression(C=i, solver='lbfgs', class_weight='balanced')        \n",
    "        cv_scores = cross_val_score(model, model_X_train, model_y_train, cv=kf)\n",
    "        \n",
    "        dict_row_score = {'score_medio':np.mean(cv_scores), 'score_std':np.std(cv_scores), 'C':i}        \n",
    "        scores_para_df.append(dict_row_score)\n",
    "\n",
    "    dfResult = pd.DataFrame(scores_para_df)\n",
    "    dfResult['limite_inferior'] = dfResult['score_medio'] - dfResult['score_std']\n",
    "    dfResult['limite_superior'] = dfResult['score_medio'] + dfResult['score_std']\n",
    "    \n",
    "    return dfResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLogisticRegressionPredictions(X_train, y_train, X_test, y_test, C, useStandarization = False):\n",
    "    \n",
    "    if(useStandarization):\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train) \n",
    "        X_test = scaler.transform(X_test) \n",
    "    \n",
    "    #para evitar el warning\n",
    "    #C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
    "    #se usa el parametro solver='lbfgs'\n",
    "    model = linear_model.LogisticRegression(C=C, solver='lbfgs', class_weight='balanced')   \n",
    "    \n",
    "    print(model)\n",
    "    \n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    return y_pred, model.score(X_test, y_test), model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metodos para Barnoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metodo para obtener el hiperparamatro alfa con el que mejor resulta el modelo Bernouilli Naive Bayes\n",
    "#useStandarization determina si se aplica o no la estandarización a los valores de los datos\n",
    "def getScoresForHypermarameterAlphaNB(valoresPosiblesAlpha, model_X_train, model_y_train, kFold_N_Splits=5, KFold_shuffle=True, useStandarization=False):\n",
    "    kf = KFold(n_splits=kFold_N_Splits, shuffle=KFold_shuffle, random_state=12)\n",
    "\n",
    "    scores_para_df = []\n",
    "    \n",
    "    #si viene configurado, se realiza la estandarización de los valores\n",
    "    if(useStandarization):\n",
    "        scaler = StandardScaler()\n",
    "        model_X_train = scaler.fit_transform(model_X_train)\n",
    "\n",
    "    for alpha in valoresPosiblesAlpha:\n",
    "        \n",
    "        \n",
    "        model = BernoulliNB(alpha=alpha)\n",
    "        cv_scores = cross_val_score(model, model_X_train, model_y_train, cv=kf)\n",
    "        \n",
    "        dict_row_score = {'score_medio':np.mean(cv_scores), 'score_std':np.std(cv_scores), 'Alpha':alpha}        \n",
    "        scores_para_df.append(dict_row_score)\n",
    "\n",
    "    dfResult = pd.DataFrame(scores_para_df)\n",
    "    dfResult['limite_inferior'] = dfResult['score_medio'] - dfResult['score_std']\n",
    "    dfResult['limite_superior'] = dfResult['score_medio'] + dfResult['score_std']\n",
    "    \n",
    "    return dfResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBernoulliNaiveBayesPredictions(bestAlpha, X_train, y_train, X_test, y_test):\n",
    "    model = BernoulliNB(alpha = bestAlpha)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    print(model)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    return y_pred, model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metodos para Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metodo para obtener el hiperparamatro alfa con el que mejor resulta el modelo Bernouilli Naive Bayes\n",
    "#useStandarization determina si se aplica o no la estandarización a los valores de los datos\n",
    "def getScoresForHypermarameterAlphaMNB(valoresPosiblesAlpha, model_X_train, model_y_train, kFold_N_Splits=5, KFold_shuffle=True, useStandarization=False):\n",
    "    kf = KFold(n_splits=kFold_N_Splits, shuffle=KFold_shuffle, random_state=12)\n",
    "\n",
    "    scores_para_df = []\n",
    "    \n",
    "    #si viene configurado, se realiza la estandarización de los valores\n",
    "    if(useStandarization):\n",
    "        scaler = StandardScaler()\n",
    "        model_X_train = scaler.fit_transform(model_X_train)\n",
    "\n",
    "    for alpha in valoresPosiblesAlpha:\n",
    "        \n",
    "        \n",
    "        model = MultinomialNB(alpha=alpha, fit_prior=True, class_prior=None)\n",
    "        cv_scores = cross_val_score(model, model_X_train, model_y_train, cv=kf)\n",
    "        \n",
    "        dict_row_score = {'score_medio':np.mean(cv_scores), 'score_std':np.std(cv_scores), 'Alpha':alpha}        \n",
    "        scores_para_df.append(dict_row_score)\n",
    "\n",
    "    dfResult = pd.DataFrame(scores_para_df)\n",
    "    dfResult['limite_inferior'] = dfResult['score_medio'] - dfResult['score_std']\n",
    "    dfResult['limite_superior'] = dfResult['score_medio'] + dfResult['score_std']\n",
    "    \n",
    "    return dfResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMultinomialNaiveBayesPredictions(bestAlpha, X_train, y_train, X_test, y_test):\n",
    "    model = MultinomialNB(alpha = bestAlpha, fit_prior=True, class_prior=None)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    print(model)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    return y_pred, model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matriz de confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>PREDICHOS</th>\n",
    "      <th>0 (F)</th>\n",
    "      <th>1 (V)</th>\n",
    "    </tr>\n",
    "      <tr>\n",
    "      <th>REALES</th>\n",
    "      <th></th>\n",
    "      <th><th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>0 (F)</td>\n",
    "      <td>TN</td>\n",
    "      <td>FN</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>1 (V)</td>\n",
    "      <td>FP</td>\n",
    "      <td>TN</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "TN: True Negative\n",
    "FN: False Negative\n",
    "FP: False Positive\n",
    "TN: True Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grafica la matriz de confusion y retorna los valores tn, fp, fn, tp de la misma\n",
    "def getConfusionMatrix(y_test, y_pred, size=5, labels = []):\n",
    "    confusionMatrix = {}\n",
    "    \n",
    "    if(labels == []):\n",
    "        confusionMatrix = confusion_matrix(y_test, y_pred)\n",
    "    else:\n",
    "        confusionMatrix = confusion_matrix(y_test, y_pred, labels)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(size,size))   \n",
    "    sns.heatmap(confusionMatrix, annot=True, fmt='d',linewidths=.5,cmap=\"Blues\")\n",
    "    ax.set_xticklabels([''] + labels)\n",
    "    ax.set_yticklabels([''] + labels)\n",
    "    plt.ylabel('Valores verdaderos')\n",
    "    plt.xlabel('Valores predichos');\n",
    "    \n",
    "    return confusionMatrix.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos relacionados a métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy, para problemas que estén equiibrados y no sesgados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelAccuracy(tn, fp, fn, tp):\n",
    "    return (tp + tn) / (tn + fp + fn + tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Para tratar de capturar la mayor cantidad de positivos posibles (cuando para el modelo es necesario capturar también los falsos positivos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelRecall(tn, fp, fn, tp):\n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelRecall(y_test, y_pred):\n",
    "    return recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Para cuando se quiere estar muy seguro de una predicción positiva. Mide la capacidad del clasificador de no etiquetar como positiva una muestra que es negativa (1 es el mejor valor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelPrecision(tn, fp, fn, tp):\n",
    "    return tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelPrecision(y_test, y_pred):\n",
    "    return precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metrica F1, cuanto mayor es su valor, mejor es el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelMetricF1(tn, fp, fn, tp):\n",
    "    precision = getModelPrecision(tn, fp, fn, tp)\n",
    "    recall = getModelRecall(tn, fp, fn, tp)\n",
    "    \n",
    "    return (2 * precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metrica F beta, similar a F1, pero puede regularse la importancia de cada termino mediante el valor de beta. SI beta > 1 => favorece al recall. Si beta < 1 => favorece a la precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelMetricFBeta(betaCoeficient, tn, fp, fn, tp):\n",
    "   \n",
    "    precision = getModelPrecision(tn, fp, fn, tp)\n",
    "    recall = getModelRecall(tn, fp, fn, tp)\n",
    "    \n",
    "    return (1 + betaCoeficient * betaCoeficient) * (precision * recall) / (betaCoeficient * betaCoeficient * precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curva ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sensitivity (True Positive Rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTruePositiveRatio(tn, fp, fn, tp):\n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specificity (False Positive Rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFalsePositiveRatio(tn, fp, fn, tp):\n",
    "    return fp / (fp + tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ploteo de curvas ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imprime la curva ROC para un modelo en particular\n",
    "def plotSingleROC_Curve(modelName, y_test, y_prob):\n",
    "    \n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    \n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.title(modelName + 'Receiver Operating Characteristic Curve')\n",
    "    plt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],linestyle='--')\n",
    "    plt.axis('tight')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imprime las curvas ROC para varios modelos\n",
    "#modelsDataDictionaryArray es un array de diccionarios con la siguiente forma\n",
    "#{\"y_prob\":<datos predichos por el modelo>, \"modelLabel\":<etiqueta para identificar el modelo en el gráfico>}\n",
    "def plotMultipleROC_Curve(y_test, modelsDataDictionaryArray):\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title('Receiver Operating Characteristic Curve')\n",
    "   \n",
    "           \n",
    "    for dictionary in modelsDataDictionaryArray:\n",
    "        false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, dictionary[\"y_prob\"])\n",
    "        roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "        label = dictionary[\"modelLabel\"] + ' - AUC = %0.2f '\n",
    "        plt.plot(false_positive_rate, true_positive_rate, label = label % roc_auc)    \n",
    "\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],linestyle='--')\n",
    "    plt.axis('tight')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
